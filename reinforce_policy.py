import numpy as npclass Policy:    def __init__(self):        self.W = np.random.normal(10, 1, size=(4, 2))         #Initializes random parameters    def choose_action(self, s, greedy=True):        s = np.reshape(s, (1, 4))        p = self.softmax(np.dot(s, self.W))        if greedy:            return np.argmax(p) #Picks the action with highest probability        else:            return np.random.choice([0, 1], 1, p=p)[0] #Picks action randomly    def softmax(self, x): #So that probabilities add to 1        x = np.reshape(x, (2,))        a = max(x)        return np.exp(x - a) / np.sum(np.exp(x - a))    def policy_gradient(self, state, action, pr=True):        grad = np.zeros((4, 2)) #4 state parameters, 2 action parameters        product = np.dot(state, self.W)        c = np.sum(np.exp(product))        if action == 0:             zero = (1 - np.exp(product[0]) / c)            grad[:, 0] = state * zero            one = np.exp(product[1]) / c            grad[:, 1] = -state * one        else:            zero = np.exp(product[0]) / c            grad[:, 0] = -state * zero            one = (1 - np.exp(product[1]) / c)            grad[:, 1] = state * one        if pr:            print('s: ', state)            print('w: ', self.W)            print('product: ', product)            print('c: ', c)            print(action, zero, one)            print('grad: ', grad)            print('')        return grad    def update(self, s, a, return_estimate, step_size):        gradient = self.policy_gradient(s, a)        self.W += step_size * gradient * return_estimate