import numpy as npimport scipy.signalimport gym# https://arxiv.org/pdf/1707.06347.pdf# https://spinningup.openai.com/en/latest/algorithms/ppo.htmlN_POS = 15N_VEL = 12class ContinuousPolicy(object):    def __init__(self, temperature=1):        self.logits = np.zeros((N_POS, N_VEL))        self.temperature = temperature        self.linear_basis = np.zeros((N_POS, N_VEL, 2))        for i, x in [(a, -1.2 + 0.128*a) for a in range(N_POS)]:            for j, v in [(a, -0.07 + 0.0127*a) for a in range(N_VEL)]:                self.linear_basis[i,j,0] = x                self.linear_basis[i,j,1] = v    def act(self, state):        f = self.f_approx(state)        sample = np.random.normal(f, self.temperature)        return np.array([sample])    def compute_gradient(self, state, action, discounted_return):        # Using formula from Silver Lecture 7, slide 18        print('doing gradient')        grad = self.feature(state) / self.temperature**2        grad *= (action - self.f_approx(state))        grad *= discounted_return / self.temperature        return grad    def gradient_step(self, grad, step_size):        self.logits += step_size * grad            def feature(self, state):        s_array = np.zeros((1,1,2))        s_array[0,0,:] = state        squared_dist = np.sum(np.square(s_array - self.linear_basis), -1)        exp = np.exp(-squared_dist/(2*self.temperature**2))        return exp            def f_approx(self, state):        return np.sum(np.multiply(self.logits, self.feature(state)))def get_discounted_returns(rewards, gamma):    r = rewards[::-1]    a = [1, -gamma]    b = [1]    y = scipy.signal.lfilter(b, a, x=r)    return y[::-1]def reinforce(env, policy):    num_episodes = 1000    gamma = 0.999    avg_rewards = 0.0          for n in range(1, num_episodes+1):        states = []        actions = []        discounted_rewards = []        rewards = []        gammas = [1]        state = env.reset()        done = False        while not done:            action = policy.act(state)            states.append(state)            actions.append(action)            state, reward, done, _  = env.step(action)            rewards.append(reward)            gammas.append(gammas[-1]*gamma)        rewards = np.array(rewards)        # print(np.sum(rewards))        avg_rewards += np.sum(rewards)         if rewards[-1] > 0:            discounted_returns = get_discounted_returns(rewards, gamma)            # print(discounted_returns)            for state, action, discounted_return, gamma_t in zip(states, actions, discounted_returns, gammas):                grad = policy.compute_gradient(state, action, discounted_return)                policy.gradient_step(grad * gamma_t, 1e-4)        if n % 100 == 0 and n > 0:            print("Episode " + str(n) + ": " + str(avg_rewards / 100))            avg_rewards = 0if __name__ == "__main__":    x_list, v_list, act_list = [], [], []        rewards = np.array([1, 1, 2])    env = gym.make('MountainCarContinuous-v0')    policy = ContinuousPolicy(0.5)    reinforce(env, policy)    state = env.reset()    x_list.append(state[0])    v_list.append    env.render()    done = False    while not done:          action = policy.act(state)          # print(action)          state, reward, done, _ = env.step(action)          env.render()            env.close()    